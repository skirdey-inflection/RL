defaults: ../../dpo.yaml
dpo:
  max_num_steps: 100
  val_period: 10
  val_batches: 1
  val_global_batch_size: 16
  reference_policy_kl_penalty: 0.1
checkpointing:
  checkpoint_dir: results/dpo-mistral-nemo-instruct-2407-1n8g-fsdp2tp8-actckpt-long
  keep_top_k: null
policy:
  model_name: mistralai/Mistral-Nemo-Instruct-2407
  tokenizer:
    name: ${policy.model_name}
  train_global_batch_size: 8
  train_micro_batch_size: 1
  max_total_sequence_length: 12288
  dtensor_cfg:
    activation_checkpointing: true
    tensor_parallel_size: 8
    clear_cache_every_n_steps: 1
    env_vars:
      PYTORCH_CUDA_ALLOC_CONF: max_split_size_mb:64
  optimizer:
    kwargs:
      lr: 1.0e-06
      weight_decay: 0.01
      betas:
      - 0.9
      - 0.999
      eps: 1.0e-08
  scheduler:
  - name: torch.optim.lr_scheduler.ConstantLR
    kwargs:
      factor: 1.0
      total_iters: 10000000000
  - milestones: []
data:
  shuffle: false
logger:
  log_dir: logs/dpo-mistral-nemo-instruct-2407-1n8g-fsdp2tp8-actckpt-long
  wandb:
    project: nemo-rl
    name: dpo-mistral-nemo-instruct-2407-1n8g-fsdp2tp8-actckpt-long
cluster:
  gpus_per_node: 8
