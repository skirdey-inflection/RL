########
# GRPO #
########

# Short 1N/1B runs (go past 200 steps - usually divergence happens by now) -- going to 4 nodes doesn't help that much
tests/test_suites/llm/grpo-qwen2.5-math-1.5b-instruct-1n8g-fsdp2tp1.v3.sh
tests/test_suites/llm/grpo-llama3.2-1b-instruct-1n8g-fsdp2tp1.v3.sh
tests/test_suites/llm/grpo-gemma3-1b-it-1n8g-fsdp2tp1.sh

# Dtensor (Qwen/Qwen2.5-7B-Instruct)
tests/test_suites/llm/grpo-qwen2.5-7b-instruct-4n8g-fsdp2tp4sp.v3.sh

# Megatron
tests/test_suites/llm/grpo-llama3.2-1b-instruct-1n8g-megatron.sh

# Functional 32b run
tests/test_suites/llm/grpo-qwen2.5-32b-32n8g-fsdp2tp8sp-actckpt.v3.sh

# Functional moonlight run 
tests/test_suites/llm/grpo-moonlight-16ba3b-4n8g-megatron.sh

# Functional VLM run
tests/test_suites/vlm/vlm_grpo-qwen2.5-vl-3b-instruct-clevr-1n2g-dtensor2tp1.v1.sh
tests/test_suites/vlm/vlm_grpo-qwen2.5-vl-3b-instruct-clevr-1n2g-megatrontp2.v1.sh

# Removing this until this issue is resolved: https://github.com/huggingface/transformers/issues/41190
# tests/test_suites/vlm/vlm_grpo-smolvlm2-2.2b-instruct-clevr-1n2g-dtensor2tp1.v2.sh

# Deepscaler (short tests)
tests/test_suites/llm/grpo-deepscaler-1.5b-16K.sh
tests/test_suites/llm/grpo-deepscaler-1.5b-24K.sh
tests/test_suites/llm/grpo-deepscaler-1.5b-8K.sh

# Deepscaler (GSPO)
tests/test_suites/llm/grpo-gspo-deepscaler-1.5b-8K.sh

# GRPO math test run (32K context mcore)
tests/test_suites/llm/grpo-math-qwen3-30ba3b-megatron-tp4-32k.sh

# FP8
tests/test_suites/llm/grpo-llama3.1-8b-instruct-1n8g-megatron-fp8-rollouts.v2.sh
tests/test_suites/llm/grpo-llama3.1-8b-instruct-1n8g-megatron-fp8-e2e.sh

# Non-colocated
tests/test_suites/llm/grpo-llama3.1-8b-instruct-2n8g-fsdp2tp1-noncolocated.sh

# Nemotron Super 49B
tests/test_suites/llm/grpo-math-llama-nemotron-super-49b-v.5-4n8g-fsdp2tp8.sh

#######
# SFT #
#######

# 1N 1B/8B runs
tests/test_suites/llm/sft-llama3.2-1b-1n8g-fsdp2tp1.v3.sh

# Dtensor (8B)
tests/test_suites/llm/sft-llama3.1-8b-1n8g-fsdp2tp2sp.sh
# dynamic batching
tests/test_suites/llm/sft-llama3.1-8b-1n8g-fsdp2tp1-dynamicbatch.sh

# Functional 32b test
tests/test_suites/llm/sft-qwen2.5-32b-4n8g-fsdp2tp8sp-actckpt.v3.sh

# Megatron
tests/test_suites/llm/sft-llama3.1-8b-1n8g-megatron.sh
# sequence packing
tests/test_suites/llm/sft-llama3.1-8b-1n8g-megatron-seqpack.sh

#######
# DPO #
#######

# 1N dtensor
tests/test_suites/llm/dpo-llama3.2-1b-instruct-1n8g-fsdp2tp1.v2.sh

# Short dtensor
tests/test_suites/llm/dpo-llama3.1-8b-instruct-4n8g-fsdp2tp2-quick.v2.sh

# Short megatron
tests/test_suites/llm/dpo-llama3.1-8b-instruct-4n8g-megatrontp2pp2-quick.sh

# Long dtensor
# Disabling until transformers upgraded to >=4.56
# Issue with details: https://github.com/NVIDIA-NeMo/RL/issues/1343
# tests/test_suites/llm/dpo-mistral-nemo-instruct-2407-1n8g-fsdp2tp8-actckpt-long.sh

################
# Distillation #
################

# Distillation tests
tests/test_suites/llm/distillation-qwen3-32b-to-1.7b-base-1n8g-fsdp2tp1.v1.sh
